# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pcyP6TMTsErZ4OcZ58we2mAi1TOJcC5S
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q tensorflow opencv-python json5

!git clone !git clone https://github.com/CubiCasa/CubiCasa5k.git

import os

# List all files and folders in the main dataset directory
dataset_path = '/content/drive/MyDrive/building_data/cubicasa5k/cubicasa5k'  # Adjust path if needed
print("Contents of dataset directory:", os.listdir(dataset_path))

# Paths to possible image folders
colorful_path = os.path.join(dataset_path, 'colorful')
high_quality_path = os.path.join(dataset_path, 'high_quality')
high_quality_arch_path = os.path.join(dataset_path, 'high_quality_architectural')

# Check each folder's contents
print("Contents of 'colorful':", os.listdir(colorful_path)[:5])  # Display first 5 items
print("Contents of 'high_quality':", os.listdir(high_quality_path)[:5])
print("Contents of 'high_quality_architectural':", os.listdir(high_quality_arch_path)[:5])

from google.colab import drive
drive.mount('/content/drive')

# Choose one sample ID folder to inspect (adjust as needed)
sample_folder = os.path.join(colorful_path, '9766')

# List contents of the sample folder
print("Contents of sample folder:", os.listdir(sample_folder))

import xml.etree.ElementTree as ET

# Path to the sample SVG file
svg_file_path = os.path.join(sample_folder, 'model.svg')

# Parse the SVG file
tree = ET.parse(svg_file_path)
root = tree.getroot()

# Print root tag and first few elements for inspection
print("SVG Root tag:", root.tag)
print("First few elements in SVG:")
for elem in list(root)[:5]:
    print(ET.tostring(elem, encoding='unicode'))

# Extract room information from SVG
rooms_info = []

for elem in root.findall('.//{http://www.w3.org/2000/svg}g[@class="Space Undefined"]'):
    room_data = {}

    # Extract room shape points
    polygon = elem.find('{http://www.w3.org/2000/svg}polygon')
    if polygon is not None:
        points = polygon.get('points')
        room_data['shape'] = points  # Store shape points as string

    # You can also extract dimensions or any other relevant info here

    # Append the room data if it contains shape information
    if room_data:
        rooms_info.append(room_data)

# Show some extracted room information
print("Extracted room information:", rooms_info[:5])  # Display first 5 rooms

!pip install Streamlit

import streamlit as st
import pickle
import numpy as np

# Load the model and label encoder
with open('floor_plan_model.pkl', 'rb') as f:
    model = pickle.load(f)
with open('label_encoder.pkl', 'rb') as f:
    label_encoder = pickle.load(f)

# Function to preprocess the input data
def preprocess_data(room_shape):
    """Preprocesses the input room shape data for prediction."""
    # Assuming room_shape is a list of coordinates
    # Apply any necessary preprocessing steps, like scaling, transformation, etc.
    # ...
    return room_shape  # Return the processed data

# Streamlit app
st.title("Floor Plan Room Classification")
st.write("Upload a floor plan SVG file to classify the rooms.")

# File uploader
uploaded_file = st.file_uploader("Choose a file", type=["svg"])

if uploaded_file is not None:
    # Process the uploaded SVG file (extract room shapes)
    # ... (use your SVG parsing logic here)
    # room_shapes = extract_room_shapes_from_svg(uploaded_file.read())

    # Example: assuming you have the room_shapes as a list of lists of coordinates
    room_shapes = [[(100, 100), (200, 100), (200, 200), (100, 200)],
                   [(300, 100), (400, 100), (400, 200), (300, 200)]]

    for room_shape in room_shapes:
        # Preprocess the data
        processed_shape = preprocess_data(room_shape)

        # Make prediction
        prediction = model.predict(np.array([processed_shape]))  # Assuming CNN model
        predicted_class = label_encoder.inverse_transform(np.argmax(prediction, axis=1))[0]

        # Display prediction
        st.write(f"Predicted room type: {predicted_class}")

def convert_shape_to_tuples(shape_str):
    # Split the string by spaces and convert to tuples of floats
    points = shape_str.strip().split()
    return [(float(x), float(y)) for x, y in (point.split(',') for point in points)]

# Convert shapes to a list of coordinates
for room in rooms_info:
    room['shape'] = convert_shape_to_tuples(room['shape'])

# Display the converted shapes
print("Converted room shapes:", rooms_info[:5])  # Show first 5 converted shapes

def normalize_coordinates(rooms, x_max=1500, y_max=1500):
    normalized_rooms = []
    for room in rooms:
        normalized_shape = [(x / x_max, y / y_max) for (x, y) in room['shape']]
        normalized_rooms.append({'shape': normalized_shape})
    return normalized_rooms

# Normalize the room shapes
normalized_rooms_info = normalize_coordinates(rooms_info)
print("Normalized room shapes:", normalized_rooms_info[:5])  # Show first 5 normalized shapes

import pandas as pd

# Assuming you have a list of room types (you can populate this based on your findings)
room_types = ["Living Room", "Kitchen", "Bedroom", "Bathroom"]  # Example types

# Create a DataFrame
data = {
    'shape': [room['shape'] for room in normalized_rooms_info],
    'type': room_types[:len(normalized_rooms_info)]  # Ensure it matches the number of rooms
}

# Adjust the length of 'type' column to match 'shape' column
# If 'room_types' is shorter than 'normalized_rooms_info',
# extend it by repeating the last element
# Otherwise, truncate 'room_types' to the length of 'normalized_rooms_info'

data['type'] = data['type'] + [data['type'][-1]] * (len(data['shape']) - len(data['type'])) if len(data['type']) < len(data['shape']) else data['type'][:len(data['shape'])]

df_rooms = pd.DataFrame(data)
print("Room DataFrame:\n", df_rooms.head())  # Display first few entries

# Check lengths
num_shapes = len(normalized_rooms_info)
num_types = len(room_types)

print("Number of room shapes:", num_shapes)
print("Number of room types:", num_types)

# Fill missing types with "Unknown"
if len(room_types) < len(normalized_rooms_info):
    room_types.extend(["Unknown"] * (len(normalized_rooms_info) - len(room_types)))

# Create the DataFrame again
data = {
    'shape': [room['shape'] for room in normalized_rooms_info],
    'type': room_types[:len(normalized_rooms_info)]  # Ensure it matches the number of rooms
}

df_rooms = pd.DataFrame(data)
print("Room DataFrame:\n", df_rooms.head())  # Display first few entries

# Trim normalized_rooms_info to match the number of room types
normalized_rooms_info_trimmed = normalized_rooms_info[:len(room_types)]

# Create the DataFrame
data = {
    'shape': [room['shape'] for room in normalized_rooms_info_trimmed],
    'type': room_types  # Use existing room types
}

df_rooms = pd.DataFrame(data)
print("Room DataFrame:\n", df_rooms.head())  # Display first few entries

!pip install matplotlib
import matplotlib.pyplot as plt # Import the matplotlib.pyplot module

def plot_rooms_with_labels(rooms):
    plt.figure(figsize=(10, 10)) # Now 'plt' is defined and can be used
    for room in rooms:
        shape = room['shape']
        xs, ys = zip(*shape)  # Unzip into x and y coordinates
        plt.fill(xs, ys, alpha=0.5)  # Fill the room shape
        # Add room type label (if needed)
        plt.text(sum(xs) / len(xs), sum(ys) / len(ys), room['type'],
                 horizontalalignment='center', verticalalignment='center')

    plt.title("Floor Plan Visualization with Room Types")
    plt.xlabel("X Coordinates")
    plt.ylabel("Y Coordinates")
    plt.axis('equal')
    plt.grid(True)
    plt.show()

# Plot the rooms with labels
plot_rooms_with_labels(df_rooms.to_dict(orient='records'))

# Check for any null or empty values in the 'shape' column
print("Null values in 'shape' column:", df_rooms['shape'].isnull().sum())
print("Empty strings in 'shape' column:", (df_rooms['shape'].str.strip() == "").sum())

# Print out the contents of the shape column to confirm structure
print("Contents of 'shape' column:")
print(df_rooms['shape'].tolist())  # Display all values in the 'shape' column

import xml.etree.ElementTree as ET
import os

# Directory path containing your SVG files
svg_folder_path = "/content/drive/MyDrive/building_data"

# Function to parse SVG and extract room shape data
def extract_room_shapes_from_svg(svg_path):
    tree = ET.parse(svg_path)
    root = tree.getroot()
    room_shapes = []

    # Namespace for SVG files
    svg_ns = {'svg': 'http://www.w3.org/2000/svg'}

    # Loop through all polygons which represent rooms
    for polygon in root.findall('.//svg:polygon', namespaces=svg_ns):
        points = polygon.get('points')
        room_shapes.append(points)  # Append raw points string for each room

    return room_shapes

# Extract data from all SVG files in the folder
all_shapes = []
for filename in os.listdir(svg_folder_path):
    if filename.endswith('.svg'):
        svg_path = os.path.join(svg_folder_path, filename)
        room_shapes = extract_room_shapes_from_svg(svg_path)
        all_shapes.extend(room_shapes)

# Print to verify extraction
print("Extracted shapes:", all_shapes)

import os

# Define the main dataset path
dataset_path = "/content/drive/MyDrive/building_data/cubicasa5k/cubicasa5k"  # Adjust this if necessary

# Explore folders in the dataset
for folder_name in ['colorful', 'high_quality', 'high_quality_architectural']:
    folder_path = os.path.join(dataset_path, folder_name)
    if os.path.exists(folder_path):
        print(f"\nContents of '{folder_name}' folder:")
        for sub_folder in os.listdir(folder_path):
            sub_folder_path = os.path.join(folder_path, sub_folder)
            if os.path.isdir(sub_folder_path):
                print(f"Files in {sub_folder}:")
                print(os.listdir(sub_folder_path))

# Filter shapes to keep only those with more coordinates, e.g., length greater than a threshold (e.g., 20 characters)
df_rooms = df_rooms[df_rooms['shape'].str.len() > 20].reset_index(drop=True)

# Display the first few rows of the filtered DataFrame
print("Filtered Room DataFrame:\n", df_rooms.head())
print("Total filtered room shapes:", len(df_rooms))

import numpy as np
from sklearn.preprocessing import LabelEncoder

# ... (Previous code for extracting and processing room shapes)

# Simulate type data (replace with your actual room type extraction/inference logic)
# This can be a list of room types corresponding to each shape in df_rooms
room_types = ['kitchen', 'bedroom', 'livingroom', 'bathroom', 'bedroom', 'livingroom', 'kitchen', 'bathroom', 'kitchen', 'livingroom', 'bedroom', 'bathroom', 'kitchen', 'bedroom', 'livingroom']
import random
room_types = [random.choice(room_types) for i in range(len(df_rooms))]
# Add the 'type' column to the DataFrame
df_rooms['type'] = room_types

# Assuming df_rooms has a 'type' column with categorical labels like "kitchen", "bedroom", etc.
# Encode these labels as numerical values using LabelEncoder

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df_rooms['type'])  # Now you can safely access 'type'

# Check the encoded labels
print("Encoded labels shape:", y.shape)
print("Sample encoded labels:", y[:5])

# Generate random labels for testing, assuming 4 room types (e.g., bedroom, kitchen, etc.)
num_classes = 4
y = np.random.randint(0, num_classes, size=X.shape[0])

# Check the generated labels
print("Random labels shape:", y.shape)
print("Sample random labels:", y[:5])

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
import numpy as np

# Check if X is empty or has incorrect dimensions
if X.ndim == 0 or X.size == 0:
    # Reshape X or handle the case where X is empty
    print("Error: X is empty or has incorrect dimensions. Please check your input data.")
    # Reshape or regenerate your X data here
    # For example, if X is meant to have 2 dimensions:
    X = X.reshape(-1, 1)  # Reshape into a column vector if it's a 1D array
    # Or if X should be generated with specific dimensions:
    # X = np.random.rand(num_samples, num_features)
else:
    # Define a basic neural network model
    model = Sequential([
        Dense(64, activation='relu', input_shape=(X.shape[1],)),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(num_classes, activation='softmax')  # Output layer with softmax for classification
    ])

    # Compile the model
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Train the model
    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

    # Evaluate the model
    test_loss, test_accuracy = model.evaluate(X_test, y_test)
    print(f"Test Accuracy: {test_accuracy:.2f}")

import pickle

import pickle
import tensorflow as tf # Import TensorFlow if not already imported

# ... (Your code for creating and training the CNN model) ...

# Assume 'model' is your trained CNN model
cnn_model = tf.keras.models.Sequential([
    # ... your model layers ...
])
# ... (Compile and train your model) ...


# Save the CNN model
with open('floor_plan_model.pkl', 'wb') as f:
    pickle.dump(cnn_model, f)

# Save the LabelEncoder
with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(label_encoder, f)

# Load the CNN model
with open('floor_plan_model.pkl', 'rb') as f:
    loaded_model = pickle.load(f)

# Load the LabelEncoder
with open('label_encoder.pkl', 'rb') as f:
    loaded_label_encoder = pickle.load(f)



